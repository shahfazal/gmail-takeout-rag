{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Newsletter Expert RAG System\n",
        "\n",
        "Building a production-ready Retrieval-Augmented Generation system for email newsletters.\n",
        "\n",
        "## Pipeline Overview\n",
        "\n",
        "1. **Preprocessing**: Extract clean text from HTML emails\n",
        "2. **Chunking**: Break emails into smaller, semantic pieces\n",
        "3. **Embedding**: Convert chunks to vectors\n",
        "4. **Indexing**: Store vectors in Qdrant\n",
        "5. **Retrieval**: Search for relevant chunks\n",
        "6. **Generation**: Answer questions using retrieved context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Our custom modules\n",
        "from src.preprocessor import EmailPreprocessor\n",
        "\n",
        "# For visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\u2705 Imports complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Email Preprocessing\n",
        "\n",
        "Let's start by extracting and cleaning text from our HTML emails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize preprocessor\n",
        "preprocessor = EmailPreprocessor()\n",
        "\n",
        "# Path to our email files\n",
        "emails_dir = \"emails/emails_to_html\"\n",
        "email_files = glob.glob(f\"{emails_dir}/*.html\")\n",
        "\n",
        "print(f\"Found {len(email_files)} email files\")\n",
        "print(f\"\\nFirst 5 files:\")\n",
        "for f in email_files[:5]:\n",
        "    print(f\"  - {Path(f).name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test with a Single Email"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract first email\n",
        "test_file = email_files[0]\n",
        "email_data = preprocessor.extract_from_html(test_file)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EXTRACTED EMAIL DATA\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Source: {Path(email_data['source']).name}\")\n",
        "print(f\"Subject: {email_data['subject']}\")\n",
        "print(f\"From: {email_data['from']}\")\n",
        "print(f\"Date: {email_data['date']}\")\n",
        "print(f\"\\nBody length: {len(email_data['body_text'])} characters\")\n",
        "print(f\"\\nBody preview (first 300 chars):\")\n",
        "print(email_data['body_text'][:300])\n",
        "print(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process All Emails (Start Small)\n",
        "\n",
        "Let's process a small batch first to see the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process first 10 emails (start small!)\n",
        "num_emails_to_process = 10\n",
        "\n",
        "processed_emails = []\n",
        "\n",
        "for email_file in email_files[:num_emails_to_process]:\n",
        "    try:\n",
        "        email_data = preprocessor.extract_from_html(email_file)\n",
        "        processed_emails.append(email_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {email_file}: {e}\")\n",
        "\n",
        "print(f\"\\n\u2705 Successfully processed {len(processed_emails)} emails\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process All Emails (Optional)\n",
        "\n",
        "Once you've tested with 10 emails and everything works, uncomment this cell to process all emails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_emails = []\n",
        "for email_file in email_files:\n",
        "    try:\n",
        "        email_data = preprocessor.extract_from_html(email_file)\n",
        "        processed_emails.append(email_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {Path(email_file).name}: {e}\")\n",
        "\n",
        "print(f\"\\n\u2705 Successfully processed {len(processed_emails)} emails\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a DataFrame for easy exploration\n",
        "df = pd.DataFrame([\n",
        "    {\n",
        "        'filename': Path(e['source']).name,\n",
        "        'subject': e['subject'][:50] + '...' if len(e['subject']) > 50 else e['subject'],\n",
        "        'from': e['from'][:30] + '...' if len(e['from']) > 30 else e['from'],\n",
        "        'body_length': len(e['body_text'])\n",
        "    }\n",
        "    for e in processed_emails\n",
        "])\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Email Body Lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize email body lengths\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(df['body_length'], bins=15, edgecolor='black')\n",
        "plt.xlabel('Email Body Length (characters)')\n",
        "plt.ylabel('Number of Emails')\n",
        "plt.title('Distribution of Email Body Lengths')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nStats:\")\n",
        "print(f\"  Average length: {df['body_length'].mean():.0f} characters\")\n",
        "print(f\"  Min length: {df['body_length'].min()} characters\")\n",
        "print(f\"  Max length: {df['body_length'].max()} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Chunking\n",
        "\n",
        "Breaking emails into smaller, overlapping chunks for better retrieval.\n",
        "\n",
        "**Why chunking?**\n",
        "- Embedding models have token limits\n",
        "- Smaller chunks = more precise retrieval\n",
        "- Better context for the LLM\n",
        "- Overlap ensures we don't lose context at boundaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Chunker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import and initialize the chunker\n",
        "from src.chunker import TokenChunker\n",
        "\n",
        "# Initialize with default settings (300 tokens per chunk, 50 overlap)\n",
        "chunker = TokenChunker(chunk_size=300, overlap=50)\n",
        "\n",
        "print(\"\u2705 Chunker initialized!\")\n",
        "print(f\"Chunk size: {chunker.chunk_size} tokens\")\n",
        "print(f\"Overlap: {chunker.overlap} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Chunking on Single Email"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with the first email\n",
        "test_email = processed_emails[0]\n",
        "chunks = chunker.chunk_email(test_email)\n",
        "\n",
        "print(f\"Email: {test_email['subject']}\")\n",
        "print(f\"Body length: {len(test_email['body_text'])} characters\")\n",
        "print(f\"Created {len(chunks)} chunk(s)\")\n",
        "\n",
        "# Show first chunk details\n",
        "if chunks:\n",
        "    print(f\"\\nFirst chunk:\")\n",
        "    print(f\"  Token count: {chunks[0]['token_count']}\")\n",
        "    print(f\"  Chunk ID: {Path(chunks[0]['chunk_id']).name}\")\n",
        "    print(f\"  Text preview: {chunks[0]['text'][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chunk All Emails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chunk all processed emails\n",
        "all_chunks = chunker.chunk_multiple_emails(processed_emails)\n",
        "\n",
        "print(f\"Total emails: {len(processed_emails)}\")\n",
        "print(f\"Total chunks created: {len(all_chunks)}\")\n",
        "print(f\"Average chunks per email: {len(all_chunks) / len(processed_emails):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explore Chunk Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrame for exploration\n",
        "chunk_df = pd.DataFrame([\n",
        "    {\n",
        "        'email': Path(c['source']).name[:40],\n",
        "        'chunk_index': c['chunk_index'],\n",
        "        'token_count': c['token_count'],\n",
        "        'subject': c['subject'][:40] + '...' if len(c['subject']) > 40 else c['subject'],\n",
        "        'text_preview': c['text'][:60] + '...'\n",
        "    }\n",
        "    for c in all_chunks\n",
        "])\n",
        "\n",
        "print(f\"Showing first 10 chunks:\\n\")\n",
        "display(chunk_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Chunk Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize token counts\n",
        "plt.figure(figsize=(10, 5))\n",
        "token_counts = [c['token_count'] for c in all_chunks]\n",
        "plt.hist(token_counts, bins=20, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('Chunk Size (tokens)')\n",
        "plt.ylabel('Number of Chunks')\n",
        "plt.title('Distribution of Chunk Token Counts')\n",
        "plt.axvline(x=300, color='r', linestyle='--', linewidth=2, label='Max chunk size (300)')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nChunk Statistics:\")\n",
        "print(f\"  Average tokens per chunk: {sum(token_counts) / len(token_counts):.1f}\")\n",
        "print(f\"  Min tokens: {min(token_counts)}\")\n",
        "print(f\"  Max tokens: {max(token_counts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Vector Database Setup\n",
        "\n",
        "Setting up **Qdrant** (Docker) to store our vector embeddings.\n",
        "\n",
        "**What's happening:**\n",
        "- Connect to Qdrant running in Docker\n",
        "- Define a \"collection\" to store email chunks\n",
        "- Each chunk will become a vector (embedding) + metadata\n",
        "- **Data persists** across notebook restarts!\n",
        "\n",
        "**Why Qdrant?**\n",
        "- Fast similarity search\n",
        "- Easy to use\n",
        "- Perfect for learning RAG\n",
        "- Can be shared with MCP servers!\n",
        "\n",
        "**Prerequisites:**\n",
        "```bash\n",
        "# Start Qdrant (already running for you):\n",
        "docker run -d -p 6333:6333 -p 6334:6334 --name qdrant-newsletter qdrant/qdrant:latest\n",
        "```\n",
        "\n",
        "**Qdrant Dashboard:** http://localhost:6333/dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Qdrant Client (Docker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams\n",
        "\n",
        "# Connect to Qdrant running in Docker\n",
        "client = QdrantClient(host=\"localhost\", port=6333)\n",
        "\n",
        "print(\"\u2705 Qdrant client initialized (Docker mode)\")\n",
        "print(\"   Connected to: http://localhost:6333\")\n",
        "print(\"   Data persists across notebook restarts!\")\n",
        "print(\"   Dashboard: http://localhost:6333/dashboard\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Collection\n",
        "\n",
        "A **collection** is like a table in a database. We'll store all our email chunk embeddings here.\n",
        "\n",
        "**Key settings:**\n",
        "- **Vector size**: 1536 (OpenAI's `text-embedding-3-small` model dimension)\n",
        "- **Distance metric**: Cosine (measures similarity between vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collection name\n",
        "COLLECTION_NAME = \"newsletter_chunks\"\n",
        "\n",
        "# Vector configuration\n",
        "# OpenAI's text-embedding-3-small produces 1536-dimensional vectors\n",
        "VECTOR_SIZE = 1536\n",
        "\n",
        "# Create the collection\n",
        "client.create_collection(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    vectors_config=VectorParams(\n",
        "        size=VECTOR_SIZE,\n",
        "        distance=Distance.COSINE  # Cosine similarity for text\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Collection '{COLLECTION_NAME}' created\")\n",
        "print(f\"   Vector size: {VECTOR_SIZE}\")\n",
        "print(f\"   Distance metric: Cosine\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check collection info\n",
        "collection_info = client.get_collection(COLLECTION_NAME)\n",
        "\n",
        "print(f\"Collection: {COLLECTION_NAME}\")\n",
        "print(f\"Vectors: {collection_info.points_count}\")\n",
        "print(f\"Status: {collection_info.status}\")\n",
        "print(f\"\\n\u2705 Qdrant is ready to receive embeddings!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Embeddings & Indexing\n",
        "\n",
        "Converting text chunks to vectors and storing them in Qdrant.\n",
        "\n",
        "**The process:**\n",
        "1. Use OpenAI's `text-embedding-3-small` model (1536 dimensions)\n",
        "2. Create embeddings for all 193 chunks\n",
        "3. Store vectors + metadata (subject, from, date, chunk ID) in Qdrant\n",
        "\n",
        "**Why embeddings?**\n",
        "- Transforms text into numbers that capture semantic meaning\n",
        "- Similar text = similar vectors (close in vector space)\n",
        "- Enables fast similarity search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import OpenAI Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from qdrant_client.models import PointStruct\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "print(\"\u2705 OpenAI client initialized\")\n",
        "print(f\"   Ready to create embeddings with text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Embeddings for All Chunks\n",
        "\n",
        "This will:\n",
        "- Take each chunk's text\n",
        "- Send it to OpenAI's embedding API\n",
        "- Get back a 1536-dimensional vector\n",
        "- Store it in Qdrant with metadata\n",
        "\n",
        "**Note:** This costs tokens! ~193 chunks \u00d7 ~165 tokens/chunk = ~32k tokens \u2248 $0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Batch size for embedding API calls\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "# Track progress\n",
        "points_to_upload = []\n",
        "total_chunks = len(all_chunks)\n",
        "\n",
        "print(f\"Creating embeddings for {total_chunks} chunks...\")\n",
        "print(f\"This will use ~{total_chunks * 165} tokens (estimated)\")\n",
        "\n",
        "# Process chunks in batches\n",
        "for i in tqdm(range(0, total_chunks, BATCH_SIZE)):\n",
        "    batch = all_chunks[i:i + BATCH_SIZE]\n",
        "    \n",
        "    # Extract text from batch\n",
        "    texts = [chunk['text'] for chunk in batch]\n",
        "    \n",
        "    # Create embeddings\n",
        "    response = openai_client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=texts\n",
        "    )\n",
        "    \n",
        "    # Prepare points for Qdrant\n",
        "    for j, chunk in enumerate(batch):\n",
        "        embedding = response.data[j].embedding\n",
        "        \n",
        "        # Create a point with vector and metadata\n",
        "        point = PointStruct(\n",
        "            id=i + j,  # Unique ID\n",
        "            vector=embedding,\n",
        "            payload={\n",
        "                \"text\": chunk[\"text\"],\n",
        "                \"source\": chunk[\"source\"],\n",
        "                \"subject\": chunk[\"subject\"],\n",
        "                \"from\": chunk[\"from\"],\n",
        "                \"date\": chunk[\"date\"],\n",
        "                \"chunk_id\": chunk[\"chunk_id\"],\n",
        "                \"chunk_index\": chunk[\"chunk_index\"],\n",
        "                \"token_count\": chunk[\"token_count\"]\n",
        "            }\n",
        "        )\n",
        "        points_to_upload.append(point)\n",
        "\n",
        "print(f\"\\n\u2705 Created {len(points_to_upload)} embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Upload to Qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload all points to Qdrant\n",
        "client.upsert(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    points=points_to_upload\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Uploaded {len(points_to_upload)} vectors to Qdrant\")\n",
        "\n",
        "# Verify upload\n",
        "collection_info = client.get_collection(COLLECTION_NAME)\n",
        "print(f\"\\nCollection now has {collection_info.points_count} vectors\")\n",
        "print(\"\ud83c\udf89 Indexing complete! Ready for retrieval.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Retrieval & Generation\n",
        "\n",
        "The RAG pipeline in action!\n",
        "\n",
        "**The flow:**\n",
        "1. **User asks a question** (e.g., \"What newsletters did I get about AI?\")\n",
        "2. **Embed the question** using OpenAI (same model as chunks)\n",
        "3. **Search Qdrant** for top-K most similar chunks (vector similarity)\n",
        "4. **Retrieve context** from those chunks\n",
        "5. **Send to LLM** with the question as a prompt\n",
        "6. **Generate answer** based on retrieved context\n",
        "\n",
        "This is the \"Retrieval-Augmented Generation\" part!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Function: Retrieve Similar Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_similar_chunks(query: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Find the most relevant chunks for a given query.\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        top_k: Number of similar chunks to retrieve\n",
        "        \n",
        "    Returns:\n",
        "        List of (chunk_text, metadata) tuples\n",
        "    \"\"\"\n",
        "    # 1. Embed the query using OpenAI\n",
        "    query_embedding = openai_client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=query\n",
        "    ).data[0].embedding\n",
        "    \n",
        "    # 2. Search Qdrant for similar vectors\n",
        "    search_results = client.query_points(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        query=query_embedding,\n",
        "        limit=top_k\n",
        "    ).points\n",
        "    \n",
        "    # 3. Extract chunks and metadata\n",
        "    results = []\n",
        "    for hit in search_results:\n",
        "        results.append({\n",
        "            \"text\": hit.payload[\"text\"],\n",
        "            \"subject\": hit.payload[\"subject\"],\n",
        "            \"from\": hit.payload[\"from\"],\n",
        "            \"date\": hit.payload[\"date\"],\n",
        "            \"score\": hit.score,  # Similarity score (0-1, higher is better)\n",
        "            \"source\": hit.payload[\"source\"]\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"\u2705 Retrieval function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Retrieval (Without LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a sample query\n",
        "test_query = \"Any emails from Instagram?\"\n",
        "\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = retrieve_similar_chunks(test_query, top_k=3)\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    print(f\"\\n\ud83d\udce7 Result {i} (score: {result['score']:.3f})\")\n",
        "    print(f\"   Subject: {result['subject']}\")\n",
        "    print(f\"   From: {result['from']}\")\n",
        "    print(f\"   Date: {result['date']}\")\n",
        "    print(f\"   Text preview: {result['text'][:200]}...\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Function: Generate Answer with LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_answer(query: str, retrieved_chunks: list):\n",
        "    \"\"\"\n",
        "    Generate an answer using retrieved chunks as context.\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        retrieved_chunks: List of relevant chunks from retrieval\n",
        "        \n",
        "    Returns:\n",
        "        Generated answer string\n",
        "    \"\"\"\n",
        "    # Build context from retrieved chunks\n",
        "    context_parts = []\n",
        "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
        "        context_parts.append(\n",
        "            f\"[Source {i}] {chunk['subject']} (from {chunk['from']}, {chunk['date']})\\n\"\n",
        "            f\"{chunk['text']}\\n\"\n",
        "        )\n",
        "    \n",
        "    context = \"\\n---\\n\".join(context_parts)\n",
        "    \n",
        "    # Create the prompt\n",
        "    prompt = f\"\"\"You are a helpful assistant that answers questions about newsletter emails.\n",
        "\n",
        "Use the following context from the user's newsletters to answer their question.\n",
        "If the context doesn't contain relevant information, say so honestly.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "    \n",
        "    # Call OpenAI\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about newsletter emails based on provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "print(\"\u2705 Generation function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete RAG Pipeline Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_newsletter_expert(query: str, top_k: int = 5, show_sources: bool = True):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: retrieve + generate.\n",
        "    \n",
        "    Args:\n",
        "        query: User's question\n",
        "        top_k: Number of chunks to retrieve\n",
        "        show_sources: Whether to display retrieved sources\n",
        "        \n",
        "    Returns:\n",
        "        Generated answer\n",
        "    \"\"\"\n",
        "    print(f\"\u2753 Question: {query}\\n\")\n",
        "    print(\"\ud83d\udd0d Retrieving relevant chunks...\")\n",
        "    \n",
        "    # Step 1: Retrieve\n",
        "    chunks = retrieve_similar_chunks(query, top_k=top_k)\n",
        "    \n",
        "    if show_sources:\n",
        "        print(f\"\\n\ud83d\udcda Found {len(chunks)} relevant chunks:\\n\")\n",
        "        for i, chunk in enumerate(chunks, 1):\n",
        "            print(f\"  {i}. {chunk['subject'][:60]}... (score: {chunk['score']:.3f})\")\n",
        "    \n",
        "    # Step 2: Generate\n",
        "    print(\"\\n\ud83d\udcad Generating answer...\\n\")\n",
        "    answer = generate_answer(query, chunks)\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"\ud83e\udd16 ANSWER:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(answer)\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return answer\n",
        "\n",
        "print(\"\u2705 Complete RAG pipeline ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the Complete RAG System! \ud83d\ude80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with your query!\n",
        "answer = ask_newsletter_expert(\n",
        "    \"Is there any instagram emails about ryan reynolds?\",\n",
        "    top_k=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Try More Queries!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try your own queries here!\n",
        "# Examples:\n",
        "# - \"Did I get any newsletters about Python programming?\"\n",
        "# - \"What job opportunities were mentioned in my newsletters?\"\n",
        "# - \"Show me newsletters about web development\"\n",
        "# - \"What newsletters did I get from [specific sender]?\"\n",
        "\n",
        "# Uncomment and modify:\n",
        "# ask_newsletter_expert(\"Your question here\", top_k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Notes & Observations\n",
        "\n",
        "Use this space to document your findings, observations, and questions as you work through the pipeline.\n",
        "\n",
        "### Questions:\n",
        "- \n",
        "\n",
        "### Observations:\n",
        "- \n",
        "\n",
        "### Next Steps:\n",
        "- Implement chunking\n",
        "- Set up Qdrant\n",
        "- Test with small dataset first"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}